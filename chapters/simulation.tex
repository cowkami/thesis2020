\documentclass[../main]{subfiles}
\begin{document}

\newpage
\chapter{シミュレーション実験}
\label{chap:simulation}

\section{シミュレーションの目的}
本シミュレーションでは, コウモリのエコーロケーションを
用いた回避行動を再現するため, エージェントのタスクは, 
"障害物に衝突せず, 一定時間移動し続けること" とした.

また, エージェントがコウモリのような効率的な飛行を行うために
制約を設ける必要がある.
そのため, コウモリの行動において, 
エネルギー消費が大きいと考えられる行動に対して, 
負の報酬を設定した.
また, 以下では負の報酬を罰則と記述する.

\section{環境とエージェント}
シミュレーションと比較対象となる行動実験では,
Fig.\ref{fig:top_view_measure}
に示す行動空間($4.5 \times 1.5$\si{m}四方)を用いた.
コウモリを図の位置から,複数個体,複数回飛行させ,
コウモリの飛行軌道とパルス放射方向を記録したデータを用いた.

シミュレーションでは, 行動実験の空間と同じ比率になるように
2次元空間を設定した.(Fig. \ref{fig:simulation_field})
エージェントはこの空間内を自由に移動できるが,
空間内の実線が壁に対応しており,
実線を交差するような移動はできず, 
このような移動を選択したとき, 
エージェントが壁に衝突したと判定する.

エージェントの初期位置は図中に示した位置を中心に,
上下左右に0.05\si{m}の範囲で,各試行毎にランダムに出現する.
これは,行動実験で生じるコウモリの飛行開始位置のばらつきを
再現するためである.

エージェントは行動開始から, 
毎ステップ進行方向に決められた距離だけ前進する.
その際,同時に以下の2つの行動を行うことができる.
\begin{itemize}
    \item 進行方向の変更
    \item パルス放射
\end{itemize}
2つの行動に関わるパラメータは,
方向変更角$\Delta\theta_t$,
パルス放射確率$p^{emit}_t$,
パルス放射角度$\phi_t$の3つで,これらをひとまとめに,
$$a_t=\{\Delta\theta_t, p^{emit}_t, \phi_t \}$$
と表記することにする.
各パラメータのとる値の範囲を
Table.\ref{tab:agent_actions}にまとめた.
ここで, $\Delta\theta_t, \phi_t$はエージェントの進行方向を
基準とした, 角度で表現される.
次に, エージェントの行動の詳細を記述する.

\subsection{進行方向の変更}
時刻$t$におけるエージェントの進行方向を$\theta_t$とする.
このとき$\theta_t$は次式によって更新される.
$$\theta_{t+1}=\theta_t+\Delta\theta_t$$
つまり, エージェントは進行方向を$\Delta\theta_t$
分変更することで自身の移動を制御する.

実世界でのコウモリの飛行は加減速を伴う.
しかし, 加減速パラメータをエージェントに組み込むと, 
制御が複雑になり, 学習コストが上昇するため, 
エージェントの速さを固定し, 進行方向を制御させた.

\subsection{パルス放射確率}
エージェントがパルス放射を行うかどうかは, 
パルス放射確率$p^{emit}_t$によって決まる.
確率を出力させた理由は, 
パルスを放射するかどうかの決定は離散的であるが, 
エージェントの行動空間が連続値であり, 
連続値と離散値を混用しないためである.
また, エージェントが, 
常にパルス放射を行ったり, 全くパルス放射を行わない, 
という行動をとるのは, 実際のコウモリと大きく乖離があるため, 
$p^{emit}_t$を
Table \ref{tab:agent_actions}に示す範囲に限定した.

\subsection{パルス放射}
コウモリは, 発したパルスのエコーを左右の耳で聞き取り, 
両耳間の時間差や, 音圧差から, エコー源を推定している
と考えられている.
エコーロケーションを行っているとき, 
環境によっては, エコーが複数存在する可能性もあるが, 
可能な限り簡単にシミュレーションするため, 
エージェントは, パルスの指向性の範囲内で, 
最も近傍のエコー源のみを, 取得することにした.
これは, エコー源が複数の場合, 
最も近傍に存在するエコー源が音圧が大きく, 
最も早く取得でき, また, 衝突の可能性が高いためである.

エージェントのパルス放射アルゴリズムを
Fig. \ref{fig:pulse_simulation}に示す.
エージェントがパルス放射を行うと, 
$\phi_t$を用いて表される, 
$[\phi_t-\frac{\pi}{6}, \phi_t+\frac{\pi}{6}]$
の領域内に存在し, 障害物として設定された点のうち, 
エージェントとの距離が最も近い座標$X=(x, y)$を, 
取得する.

\subsection{指向性}
コウモリのパルスは鋭い指向性をもっている.
そのため, パルス放射方向と大きく異なる方向からの
エコー音圧は非常に弱くなる.
本シミュレーションでは, パルス放射方向を中心に, 
エージェントの視点から左右に$\pi/6$
開いた範囲外の情報は無視した.

\subsection{記憶パラメータ}
パルス放射によって, エージェントは自身を基準とした, 
障害物の座標$X_t$を得た. 
エージェントは常に直近の$n$個を保持し, 
$$
s_t = \{X_{t-n+1}, X_{t-n+2}, ..., X_t\}
$$
この$s_t$を時刻$t$における状態とした.
また, 保持する$X$の数$n$を記憶パラメータと呼び, 
$n=5$に設定した

\subsection{報酬設計}
報酬関数は次のように設定した. 
ただし, 衝突判定変数$B$はエージェントが衝突したときに, $B=1$となり, 
それ以外では$B=0$である.

\begin{equation}
\label{eq:reward_func}
 g(a_t) = 
w_1 +
w_2 \frac{|\Delta\theta_t|}{max \Delta\theta} +
w_3 p^{emit}_t +
w_4 \frac{|\phi_t|}{max \phi} +
w_5 B
\end{equation}

上式の重みパラメータ$w_i$の具体的な値は
Table. \ref{tab:reward_weights}に示した.

式(\ref{eq:reward_func})の第一項は, 
継続的な行動を評価するため, 無条件に$\omega_1$を与え続けた.
本シミュレーションのタスクは, 
エージェントが移動を続けることであるので, 
移動が続く限り一定の報酬を与えることで, 学習を促進した.
$w_1$の大きさは, 強化学習の課題としてよく使われる, 
継続的タスクの報酬設計を参考に$w_1=1$と定めた.

第二から第四項は, 行動そのものへの罰則である.
行動パラメータはそれぞれ値の採る範囲が異るので, 
$[0, 1]$の範囲で正規化している.
さらに負の値をとるパラメータは絶対値をとっており, 
その大きさに比例して, 罰則が大きくなる.
エージェントの移動経路に関わる第二項は$w_2=10$, 
パルス放射に関わる第三, 四項は$w_3=w_4=10^{-3}$
とした.

第五項は, エージェントが障害物に衝突したときに与える罰則である.
$w_5=-100$と, 他の罰則と比較して最も大きな値を定めた.

ここで, 各値の大小関係
$$
w_1>w_3=w_4>w_2>w_5
$$
が, コウモリの効率的な飛行における
パラメータの重要度であると仮定した.

\subsection{学習時間}


\section{実験結果}
\section{考察}
本研究ではエージェントは学習が進むに従い, 
消費エネルギーを減少させるように行動しているという仮定を
元に報酬設計を行い, シミュレーションを行った.
結果, 飛行軌跡, パルス放射回数の変化は行動実験と同様の傾向が現れた.
この結果により,
コウモリも空間の学習が進むにしたがって
より消費エネルギーを減少させる行動を選択していると考えられる.
対して, パルス放射角度は行動実験とは異なり, 
シミュレーションでは飛行方向から大きな角度をつけ放射する傾向となった.
これより, コウモリは, シミュレーションの設定のように, 
最近傍点1点のみを見ているのではなく.
放射パルスの指向性をうまく活用し, 
環境把握ができているのではないかと推測される.

\end{document}