\documentclass[../main]{subfiles}
\begin{document}

\newpage
\chapter{シミュレーション実験}
\label{chap:simulation}

\section{シミュレーションの目的}
本シミュレーションでは, コウモリのエコーロケーションを
用いた回避行動を再現するため, エージェントのタスクは, 
"障害物に衝突せず, 一定時間移動し続けること" とした.

また, エージェントがコウモリのような効率的な飛行を行うために
制約を設ける必要がある.
そのため, コウモリの行動において, 
エネルギー消費が大きいと考えられる行動に対して, 
負の報酬を設定した.
また, 以下では負の報酬を罰則と記述する.

\section{環境とエージェント}
シミュレーションと比較対象となる行動実験では,
Fig.\ref{fig:top_view_measure}
に示す行動空間($4.5 \times 1.5$\si{m}四方)を用いた.
コウモリを図の位置から,複数個体,複数回飛行させ,
コウモリの飛行軌道とパルス放射方向を記録したデータを用いた.

シミュレーションでは, 行動実験の空間と同じ比率になるように
2次元空間を設定した.(Fig. \ref{fig:simulation_field})
エージェントはこの空間内を自由に移動できるが,
空間内の実線が壁に対応しており,
実線を交差するような移動はできず, 
このような移動を選択したとき, 
エージェントが壁に衝突したと判定する.

エージェントの初期位置は図中に示した位置を中心に,
上下左右に0.05\si{m}の範囲で,各試行毎にランダムに出現する.
これは,行動実験で生じるコウモリの飛行開始位置のばらつきを
再現するためである.

エージェントは行動開始から, 
毎ステップ進行方向に決められた距離だけ前進する.
その際同時に2つの行動, 
進行方向の変更と,
パルス放射を行うことができる.

2つの行動に関わるパラメータは,
方向変更角$\Delta\theta_t$,
パルス放射確率$p^{emit}_t$,
パルス放射角度$\phi_t$の3つで,これらをひとまとめに,
$$a_t=\{\Delta\theta_t, p^{emit}_t, \phi_t \}$$
と表記することにする.
各パラメータのとる値の範囲を
Table.\ref{tab:agent_actions}にまとめた.
ここで, $\Delta\theta_t, \phi_t$はエージェントの進行方向を
基準とした, 角度で表現される.
次に, エージェントの行動の詳細を記述する.

\subsection{進行方向の変更}
時刻$t$におけるエージェントの進行方向を$\theta_t$とする.
このとき$\theta_t$は次式によって更新される.
$$\theta_{t+1}=\theta_t+\Delta\theta_t$$
つまり, エージェントは進行方向を$\Delta\theta_t$
分変更することで自身の移動を制御する.

実世界でのコウモリの飛行は加減速を伴う.
しかし, 加減速パラメータをエージェントに組み込むと, 
制御が複雑になり, 学習コストが上昇するため, 
エージェントの速さを固定し, 進行方向を制御させた.

\subsection{パルス放射確率}
エージェントがパルス放射を行うかどうかは, 
パルス放射確率$p^{emit}_t$によって決まる.
確率を出力させた理由は, 
パルスを放射するかどうかの決定は離散的であるが, 
エージェントの行動空間が連続値であり, 
連続値と離散値を混用しないためである.
また, エージェントが, 
常にパルス放射を行ったり, 全くパルス放射を行わない
行動をとるのは, 実際のコウモリと大きく乖離があるため, 
$p^{emit}_t$を
Table \ref{tab:agent_actions}に示す範囲に限定した.

\subsection{パルス放射}
コウモリは, 発したパルスのエコーを左右の耳で聞き取り, 
両耳間の時間差や, 音圧差から, エコー源を推定している
と考えられている.
エコーロケーションを行っているとき, 
環境によっては, エコーが複数存在する可能性もあるが, 
可能な限り簡単にシミュレーションするため, 
エージェントは, パルスの指向性の範囲内で, 
最も近傍のエコー源のみを取得することにした.
これは, エコー源が複数の場合, 
最も近傍に存在するエコー源が音圧が大きく, 
最も早く取得でき, また, 衝突の可能性が高いためである.

エージェントのパルス放射アルゴリズムを
Fig. \ref{fig:pulse_simulation}に示す.
エージェントがパルス放射を行うと, 
$\phi_t$を用いて表される, 
$[\phi_t-\frac{\pi}{6}, \phi_t+\frac{\pi}{6}]$
の領域内に存在し, 障害物として設定された点のうち, 
エージェントとの距離が最も近い座標$X=(x, y)$を取得する.

\subsection{指向性}
コウモリのパルスは鋭い指向性をもっている.
そのため, パルス放射方向と大きく異なる方向からの
エコー音圧は非常に弱くなる.
本シミュレーションでは, パルス放射方向を中心に, 
エージェントの視点から左右に$\pi/6$
開いた範囲外の情報は無視した.

\subsection{記憶パラメータ}
パルス放射によって, エージェントは自身を基準とした, 
障害物(エコー源)の座標$X_t$を得た. 
しかし, 回避行動において, 現在のエコー源座標だけでは, 
情報が不足しており, 時系列を反映させる必要があると考え, 
エージェントに記憶パラメータを設けた.

エージェントは常に直近の座標を決められた数$n$個保持する. 
これを, 
$$
s_t = \{X_{t-n+1}, X_{t-n+2}, ..., X_t\}
$$
と表し, $s_t$を時刻$t$における状態とした.
また, $n$を記憶パラメータと呼び, $n=5$に設定した.

\subsection{報酬設計}
報酬設計は, 本実験において最も重要なパラメータである.
飛行実験によって得られた結果から, 各パラメータの重要度を推定し, 
それに基づいた報酬設計を行い, エージェントを学習させる. 
エージェントが, コウモリに類似した行動をとれば, 
パラメータに関する推定が支持されると考えられる.

本シミュレーションの結果の比較対象となる, 
コウモリの行動実験のデータを示す.
キクガシラコウモリを4個体, Fig. \ref{fig:top_view_measure}
の空間で12回ずつ飛行させ, 
飛行軌跡と, パルス放射方向を記録し, 
初回飛行時と最終飛行時の記録を, 
Fig. \ref{fig:bat_trajectories}に示した.
空間習熟後の飛行では, 初回飛行時より小さな蛇行幅で
障害物を回避している. 
また, パルスを飛行方向に対して, 
小さな角度で放射する傾向が見られた.
次に, コウモリが飛行を開始してから, 
3枚目の障害物を回避するまでに行った, 
パルス放射回数をFig. \ref{fig:bat_pulse}
に示した.
全ての個体で, 空間習熟後におけるパルス放射回数の減少がみられた.
これらの結果から, コウモリは空間の学習が進みに従い, 
消費エネルギーを減少させ障害物飛行をしている可能性が示唆された.

これらの結果から, 報酬関数は次のように設定した. 
ただし, 衝突判定変数$B$はエージェントが衝突したときに, $B=1$となり, 
それ以外では$B=0$である.

\begin{equation}
\label{eq:reward_func}
 g(a_t) = 
w_1 +
w_2 \frac{|\Delta\theta_t|}{max \Delta\theta} +
w_3 p^{emit}_t +
w_4 \frac{|\phi_t|}{max \phi} +
w_5 B
\end{equation}

上式の重みパラメータ$w_i$の具体的な値は
Table. \ref{tab:reward_weights}に示した.

式(\ref{eq:reward_func})の第一項は, 
継続的な行動を評価するため, 無条件に$\omega_1$を与え続けた.
本シミュレーションのタスクは, 
エージェントが移動を続けることであるので, 
移動が続く限り一定の報酬を与えることで, 学習を促進した.
$w_1$の大きさは, 強化学習の課題としてよく用いられる, 
継続的タスクの報酬設計を参考に$w_1=1$と定めた.

第二から第四項は, 行動そのものへの罰則である.
行動パラメータはそれぞれ値の採る範囲が異なるので, 
$[0, 1]$の範囲で正規化している.
さらに負の値をとるパラメータは絶対値をとっており, 
その大きさに比例して, 罰則が大きくなる.
エージェントの移動経路に関わる第二項は$w_2=10$, 
パルス放射に関わる第三, 四項は$w_3=w_4=10^{-3}$とした. 

第五項は, エージェントが障害物に衝突したときに与える罰則である.
$w_5=-100$と, 他の罰則と比較して最も大きな値を定めた.

以上の重みパラメータを設定する際は, 
次の大小関係が成り立つようにした.
$$
w_1>w_3=w_4>w_2>w_5
$$
この関係は, コウモリの効率的な飛行における, 
パラメータの仮定に対応している.


\subsection{終了条件}
エージェントが移動を始めてから, 
ある条件に達するとその試行を終了する.
終了条件は2つあり, 1つ目はエージェントが壁に衝突すること, 
2つ目は, 決められたステップ数が経過することである.
このステップ数は, エージェントの軌道の長さが\SI{5}{\metre}になるよう
1000ステップに設定した. 
行動空間の$x$軸方向の長さが\SI{4.5}{\metre}であるので, 
エージェントが蛇行しながら移動をしても
十分に空間の右端にたどり着くことが可能である.


\subsection{学習回数}
強化学習によるエージェントの行動は, 
初期値がランダムになっており, 
学習初期では回避行動をとることが困難である.
そこで, 回避行動を十分に学習したエージェントと, 
さらに学習を続けることでタスクへの最適化が進んだエージェントの
2つの学習済みモデルを用意した.
ここで, 回避行動を十分に学習したモデルを初回飛行モデル, 
最適化がさらに進んだモデルを最終飛行モデル, と呼称し, 
行動実験の, 初回飛行と最終飛行のデータにそれぞれ対応させ, 
比較, 考察を行った.

学習の際, エピソード数を固定する方法と, 
各エピソードの合計ステップ数を固定する方法があり, 
エピソード毎のステップ数が大きく変動するタスクでは, 
合計ステップ数を固定するほうが相性がよいため, これを採用した. 
初回飛行モデルは, 合計$1\times 10^6$ステップ,
最終飛行モデルは, さらに続けて$1\times 10^6$
(合計$2\times 10^6$)ステップ学習させた.


\end{document}