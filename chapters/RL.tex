\documentclass[../main]{subfiles}
\begin{document}

\newpage
\chapter{強化学習}
\label{chap:RL}
強化学習とは機械学習の枠組みの一つであり, 
Deep Q-Learning\cite{ref:dqn}の成功によって, 
深層学習を取り入れた強化学習の手法が急速に発展している.

強化学習では, ある制御対象のシステムを環境, 
その環境内での制御者のことをエージェントと呼ぶ.
強化学習の目的は, エージェントが環境内で最適な方策を求めることである.
エージェントは環境に対する情報をほとんど持っておらず, 
環境内で行動を繰り返していくことで, 
自ら環境のデータを収集し, 最適な方策を探索していく.

\section{マルコフ決定過程}
強化学習では, 状態が時間遷移していく確立過程を扱う.
ある時間ステップ$t$における確率変数を$X_t$, 
$X_t$がとり得る全ての値の集合を$\chi$としたとき, 
一般の確率過程では, $X_t$が$x\in\chi$をとる確率は, 
$$
P(X_t=x|X_1=x_1, ... ,X_{t-1}=x_{t-1})
$$
のように, 過去の全ての実現値に依存する.
しかし, 現実では状態数が多数の場合が多く, 
組み合わせが膨大になり計算が困難なため, 
マルコフ性という仮定をおく.
マルコフ性とは,
$$P(X_{t+k}=x|X_t=x_t)$$
と表され, 将来の$X_{t+k}$の確率分布が, 
現在の値$x_t$にのみ依存する, という性質である.
特にマルコフ性を満たす確率過程のうち, 
状態(state)が離散値をとるものをマルコフ連鎖と呼ぶ.

マルコフ決定過程とは, マルコフ連鎖に, 
行動(action)と報酬(reward)を追加したもので, 
時刻$t$における状態が$s_t\in S(Sは状態の有限集合)$のとき
行動決定者(エージェント)が





\section{問題設定}
期待報酬を最大化

\section{方策の探索}
方策を直接求める.

エージェントはある時刻$t$において, 環境から状態$s_t$を得る.
環境に対して方策$\pi(\cdot|s_t)$に従って, 
行動$a_t$を起こすことで報酬関数$g(s_t,a_t)$による報酬$r_t$と, 
次の状態$s_{t+1}$が得られる. 
報酬を最大化する最適な方策を求めることが強化学習の目的である.


\section{試行}
制御対象が, ある一連の動作を繰り返すようなタスクを行う場合, 
試行には


\section{Policy Proximal Optimization(PPO)}
本実験では，強化学習アルゴリズムとして, 
Policy Proximal Optimization(PPO)\cite{ref:proximal_policy}を用いた.
PPOは方策$\pi$を陽に求める方策勾配法と, Actor-Critic

\end{document}