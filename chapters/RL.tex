\documentclass[../main]{subfiles}
\begin{document}

\newpage
\chapter{強化学習}
\label{chap:RL}
強化学習とは機械学習の枠組みの一つであり, 
Deep Q-Learning\cite{ref:dqn}の成功によって, 
深層学習を取り入れた強化学習の手法が急激に発展している.

強化学習では, ある制御対象のシステムを環境, 
その環境内での制御者のことをエージェントと呼ぶ.
強化学習の目的は, エージェントが環境内で最適な方策を求めることである.
エージェントは環境に対する情報をほとんど持っておらず, 
環境内で行動を繰り返していくことで, 
自ら環境のデータを収集し, 最適な方策を探索していく.

\section{マルコフ決定過程}
強化学習では, 状態が時間遷移していく確立過程を扱う.
ある時間ステップ$t$における確率変数$X_t$, 
確率変数$X_t$がとり得る全ての値の集合を$\chi$, 
としたとき, $X_t$が$x\in\chi$をとる確率は, 
$$
P(X_t=x|X_1=x_1, ... ,X_{t-1}=x_{t-1})
$$
のように, 過去の全ての実現値に依存する.
しかし, 現実では状態数が多数の場合が多く, 
計算量が膨大になるため, 
マルコフ性という仮定をおく.


\section{方策の探索}
 

\section{}

エージェントはある時刻$t$において, 環境から状態$s_t$を得る.
環境に対して方策$\pi(\cdot|s_t)$に従って, 
行動$a_t$を起こすことで報酬関数$g(s_t,a_t)$による報酬$r_t$と, 
次の状態$s_{t+1}$が得られる. 
報酬を最大化する最適な方策を求めることが強化学習の目的である.


本実験では，強化学習アルゴリズムとして, 
PPO(Proximal Policy Optimization)
\cite{ref:proximal_policy}を用いた.

\end{document}