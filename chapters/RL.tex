\documentclass[../main]{subfiles}
\begin{document}

\newpage
\chapter{強化学習}
\label{chap:RL}
強化学習とは機械学習の枠組みの一つであり, 
Deep Q-Learning\cite{ref:dqn}の成功によって, 
深層学習を取り入れた強化学習の手法が急速に発展している.

強化学習では, ある制御対象のシステムを環境, 
その環境内での行動決定者のことをエージェントと呼ぶ.
強化学習の目的は, 
エージェントが環境内で最適な方策を求めることである.
エージェントは環境に対する情報をほとんど持っておらず, 
環境内で行動を繰り返していくことで, 
自ら環境のデータを収集し, 最適な方策を探索していく.

\section{マルコフ決定過程}
強化学習では, 状態が確率的に時間遷移していく確立過程を扱う.
ある時間ステップ$t$における確率変数を$X_t$, 
$X_t$がとり得る全ての値の集合を$\chi$としたとき, 
一般の確率過程では, $X_t$が$x\in\chi$をとる確率は, 
\begin{equation}
P(X_t=x|X_1=x_1, ... ,X_{t-1}=x_{t-1})
\end{equation}
のように, 過去の全ての実現値に依存する.
しかし, 現実のタスクでは状態数が多数であるが故に, 
状態変化の組み合わせが膨大になり, 計算が困難になることがある.
そこで, 確率過程にマルコフ性という仮定をおく.
マルコフ性とは, 将来の状態$X_{t+k}$の確率分布が, 
現在の値$x_t$にのみ依存する, という性質である.
\begin{equation}
P(X_{t+k}=x|X_t=x_t)
\end{equation}
特にマルコフ性を満たす確率過程のうち, 
状態(state)が離散値をとるものをマルコフ連鎖と呼ぶ.

マルコフ決定過程とは, マルコフ連鎖に, 
行動(action)と報酬(reward)を追加したものを指す.
マルコフ決定過程において, 
時刻$t$のとき, 状態を$s_t \in S(Sは状態の有限集合)$, 
エージェントがとる行動を
$a_t \in \mathcal{A}(\mathcal{A}は行動の有限集合)$, 
と定める. 

\subsection{方策}
状態が$s_t$が与えられたときに, 
エージェントが行動$a_t$をとる条件付き確率分布を
確率的方策関数と定め, 方策パラメータ
$\theta \in \mathcal{R}^n$を用いて, 
\begin{equation}
\pi_\theta(a_t|s_t)
\end{equation}
と表す. また, 行動が連続値であるとき($a \in \mathcal{R}^n$),
方策$\pi_\theta$は確率密度関数である.

方策はエージェントの意思決定の基準であり, 
エージェントが最適な行動をとるためには, 
最適な方策パラメータを求めればよい.


\subsection{報酬}
状態が$s_t$のときに, エージェントは
方策$\pi_\theta(\cdot|s_t)$によって行動$a_t$を選択する.
この際, 獲得できる報酬は, 
事前に設定した報酬関数$g(s_t, a_t)$によって定まる. 
最適な方策を評価する基準は, 報酬によって決定する.


\section{目的関数}
$g(s_t, a_t)$の出力結果を$R_t \in \mathcal{R}$とする.
事前に設定されたパラメータ
割引率$\gamma \in [0, 1)$を用いて, 

\section{問題設定}
期待報酬を最大化

\section{方策の探索}
方策を直接求める.

エージェントはある時刻$t$において, 環境から状態$s_t$を得る.
環境に対して方策$\pi(\cdot|s_t)$に従って, 
行動$a_t$を起こすことで報酬関数$g(s_t,a_t)$による報酬$r_t$と, 
次の状態$s_{t+1}$が得られる. 
報酬を最大化する最適な方策を求めることが強化学習の目的である.


\section{試行}
制御対象が, ある一連の動作を繰り返すようなタスクを行う場合, 
試行には


\section{Policy Proximal Optimization(PPO)}
本実験では，強化学習アルゴリズムとして, 
Policy Proximal Optimization(PPO)
\cite{ref:proximal_policy}を用いた.
PPOは方策$\pi$を陽に求める方策勾配法と, Actor-Critic

\end{document}