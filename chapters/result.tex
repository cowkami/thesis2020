\documentclass[../main]{subfiles}
\begin{document}

\newpage
\chapter{考察とまとめ}
\label{chap:result}

\section{考察}
本研究では, 報酬設計を含む, 適切なシミュレーション環境において, 
強化学習によって獲得される方策に基づくエージェントの行動は, 
実物のコウモリの行動と類似した結果になることが期待されている.
そのため, エージェントの行動と, 行動実験の結果を比較し, 
類似点と相違点に注目して考察を行った. 
また, シミュレーションをより現実に近づけるために
必要となる要素や工夫について検討した.


\subsection{軌道}
コウモリはエージェントと異なり, 
左右方向への方向転換のみではなく, 
加速や減速をしながら飛行できる.
本シミュレーションでは, エージェントの行動をシンプルにするため, 
進行方向を選択させたが, 
一定時間学習後のエージェントの回避行動は十分に
コウモリを模擬しているように思える.
また, 回避行動を獲得してからの学習による変化も
行動実験と類似していることから
進行方向変更角度に対する報酬の設定が,
ある程度の妥当性があったと考える.


\subsection{パルス放射}
エージェントのパルス放射の意思決定は, 
強化学習の実装上の都合から, 
確率によって決定した.
実際のコウモリがどのような意思決定によって 
パルス放射を行うのかは, 
コウモリの主観によるので, 言及できない.
しかし, 学習によるパルス放射回数の減少という点では, 
軌道同様, ある程度の妥当性が見られる.

パルス放射方向に関しては, 
類似した行動が見られなかったが, 
3つの理由が考えられる.

1つ目は, エージェントに与える罰則が小さすぎたことである.
パルス放射に関する報酬設定は少し複雑で, 
パルス放射が行われたときのみ, 
パルス放射への罰則と, パルス放射方向への罰則が与えられる.
そのため, パルス放射方向の罰則をパルス放射への罰則と同じ値に設定した.
しかし, 他の報酬と比較して非常に小さくなるため, 
学習への影響が少なかったと考えられる.

2つ目は, 環境に過適合したことが考えられる.
本実験設定では, エージェントは移動開始から, 
殆ど壁に衝突しない方向に動き出す.
そのため, エージェントの視点からみて, 
正面に障害物が現れる可能性は低く, 
エージェントは斜め前からの障害物, つまり, 
交互に設定した線分のみを避ければ良い.
よって, 左右に多くパルス放射を行ったのではないかと考えられる.

3つ目は, 実際のコウモリが行っているエコーロケーションと, 
シミュレーションの乖離である.
本実験では, コウモリのエコーロケーションによる
空間把握に関して, エージェントからの最近傍の
エコー源を取得する, という非常に限定的な情報をエージェントに与えた.
しかし, コウモリは周波数特性をうまく活用して, 
一点ではなく, もっと広い範囲の情報を把握できていることが示唆された.
また, このような行動は行動実験\cite{ref:miwasan}
によっても確認されており, 
次の課題として, 本研究手法を用いて, 
コウモリが探索範囲を上手く活用する行動の, 
行動理由を説明することが期待できる.


\section{まとめ}
エージェントの行動に関して, 飛行軌跡, 
パルス放射回数の変化は行動実験と同様の傾向が現れた.
この結果により,
コウモリも空間の学習が進むにしたがって
より消費エネルギーを減少させる行動を選択していると考えられる.
対して, パルス放射角度は行動実験とは異なり, 
シミュレーションでは飛行方向から大きな角度をつけ放射する傾向となった.
これは, 報酬設計や環境への過適合などが原因とも考えられるが, 
その他の原因として, コウモリは, シミュレーションの設定のように, 
最近傍点1点のみを見ているのではなく.
放射パルスの指向性をうまく活用し, 
環境把握ができているのではないかと推測される.

    
\end{document}