\documentclass[../main]{subfiles}
\begin{document}

\newpage
\chapter{結果と考察}
\label{chap:result}
\section{実験結果}
\subsection{学習結果}
Figure \ref{fig:avearge_reward}は
エージェントが獲得した, 
1000ステップ毎の平均報酬の推移である.
学習初期には急激に上昇し, 以降緩やかになっていることから, 
エージェントがある程度タスクに習熟したことが示唆される.

\subsection{移動軌道とパルス放射}
$1\times 10^6$ステップ学習済みモデルと, 
$2\times 10^6$ステップ学習済みモデルの,
移動軌跡とパルス放射方向をFig. \ref{fig:agent_trajectory}
に示す.
行動実験同様に, 学習によって蛇行幅が小さくなっていることがわかる.
また, Fig. \ref{fig:agent_pulse}は2つのモデルのエージェントで, 
100回ずつ試行を繰り返し, 3枚目の障害物を回避するまでに, 
発したパルス放射回数の平均である.
こちらも, 行動実験同様, パルス放射の減少が確認できる.
しかし, 行動実験とは異なり, エージェントのパルス放射方向は, 
学習が進むに従って, 大きくなる傾向がみられた.


\section{考察}
本研究では, 報酬設計を含む, 適切なシミュレーション環境において, 
強化学習によって獲得される方策に基づくエージェントの行動は, 
実物のコウモリの行動と類似した結果になることが期待されている.
そのため, エージェントの行動と, 行動実験の結果を比較し, 
類似点と相違点に注目して考察を行った. 
また, シミュレーションをより現実に近づけるために
必要となる要素や工夫について検討した.

\subsection{軌道}

\subsection{パルス放射回数}

\subsection{パルス放射方向}

\section{まとめ}
エージェントの行動に関して, 飛行軌跡, 
パルス放射回数の変化は行動実験と同様の傾向が現れた.
この結果により,
コウモリも空間の学習が進むにしたがって
より消費エネルギーを減少させる行動を選択していると考えられる.
対して, パルス放射角度は行動実験とは異なり, 
シミュレーションでは飛行方向から大きな角度をつけ放射する傾向となった.
これより, コウモリは, シミュレーションの設定のように, 
最近傍点1点のみを見ているのではなく.
放射パルスの指向性をうまく活用し, 
環境把握ができているのではないかと推測される.

    
\end{document}